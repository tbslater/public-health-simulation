[
  {
    "objectID": "posts/computational-reproducibility/computational-reproducibility-blog-post.html",
    "href": "posts/computational-reproducibility/computational-reproducibility-blog-post.html",
    "title": "Reproducibility matters",
    "section": "",
    "text": "Reproducibility is the gold standard of research. I would argue it is not only important, but essential for scientific work and it is something that can certainly be achievable. Keep reading to find out what reproducibility is, why it is crucial and how you can make your own computational work more reproducible.\nTime to read: 4 minutes\n\n\n\nhttps://unsplash.com/photos/a-computer-screen-with-a-bunch-of-code-on-it-ieic5Tq8YMk (C.Ried, 2018)\n\n\nWHAT IS COMPUTATIONAL REPRODUCIBILITY?\nResearch is said to be computationally reproducible if its results can be obtained from the data, methods and code outlined by the author(s) [1].\nIn other words, if another researcher - separate from the original team - can reach the same outputs using the exact same approach, then full reproducibility has been achieved. If only some results can be obtained, we may refer to this as partial reproducibility.\nWHY SHOULD WE BE BOTHERED ABOUT REPRODUCIBILITY?\nZiemann et al. rightly stress the negative implications of work that cannot be reproduced [2] (I highly recommend reading their paper). At the very least, irreproducible research can stunt development. It is much harder to share work with minimal or even no attempt to outline the methods and code. Learning from this becomes impossible. More dangerously, irreproducible work cannot be verified and so its credibility remains doubtful. Without trust in one’s work, increased resistance between research and policymakers is to be expected, and even if the work is accepted, how do we know acting on the conclusions is safe, if it has not been verified? Needless to say, reproducible research is vital.\nHOW CAN COMPUTATIONAL REPRODUCIBILITY BE ASSESSED?\nReproducibility can be assessed by working through a paper’s methods and running any associated code to obtain the key results [1]. If consistent, though not necessarily identical, outputs are achieved, then the research can be deemed reproducible. This should be done within a reasonable time-frame, at the discretion of the individual researcher.\nREPRODUCIBILITY VS REPLICABILITY\nReplicability is often mentioned in conjunction with reproducibility, but the two should not be confused. Replicable work refers to the achievement of consistent results across several studies, possibly using different methods and data [1]. On the contrary, reproducibility is strictly concerned with use of the same methods. Both are certainly important for the advancement of science, but attaining replicable work is somewhat out of a researcher’s control. Reproducible work should, however, always be expected if best practices have been followed.\nTIPS TO IMPROVE COMPUTATIONAL REPRODUCIBILITY\nBeing accountable to even simple rules can drastically improve computational reproducibility. Researchers should [3] (if possible):\n\nShare code and data: code and data should be made publicly available, follow coding guidelines and ought to run without much modification required.\nUse random seeds: for stochastic models, seeds should be used to ensure randomness is consistent.\nSpecify package versions: packages change over time and can render code to become outdated (and thus not reproducible).\nFully detail methods: the computational model should be explained in its entirety.\nProvide an ORCID iD: authors with ORCID iD can be more easily identified and contacted [4].\n\nThis list is certainly not exhaustive. More rigorous reproducibility frameworks exist for specific models and I encourage you to look at any relevant to you. For simulation models check out https://doi.org/10.1080/17477778.2018.1442155 [5].\nFREE AND OPEN SOURCE SOFTWARE\nOpting for free and open source software (FOSS) is essential where reproducibility is concerned. Under FOSS, users are able to share and modify code freely [6]. Naturally, this leads to more collaborative research and crucially allows others to develop and verify computational results. Researchers should therefore aim to use FOSS where possible.\nARE WE IN A STATE OF CRISIS?\nWith a vast existence of irreproducible work, science finds itself deep into a ‘reproducibility crisis’ [7,8], or so people say. But is it really that serious? It certainly was nearly a decade ago: in 2016, Nature found that over 70% of reproduction attempts were failed by scientists [9], highlighting just how bad the problem was. Nine years on and the issue is still very relevant [10], though the extent to which is unclear. Nonetheless, action is still required now and so if it helps to highlight the severity of irreproducible research, then I will gladly agree that we are in a crisis.\nFINAL REMARKS\nI hope this post has convinced you that reproducible work does indeed matter. The reproducibility crisis can be tackled, but it starts with individual accountability. So start now!\nREFERENCES\n\nUnderstanding Reproducibility and Replicability. National Academies of Sciences, Engineering, and Medicine. 2019. Reproducibility and Replicability in Science.\nZiemann, M., Poulain, P., and Bora, A., The five pillars of computational reproducibility: bioinformatics and beyond. Brief Bioinform, 2023. 24(6).\nSandve, G.K., et al., Ten simple rules for reproducible computational research. PLoS Comput Biol, 2013. 9(10): p. e100328\nORCID iDs at Springer Nature. [Accessed: 6 March 2025]; Available from: https://www.springernature.com/gp/researchers/orcid.\nMonks, T., et al. Strengthening the reporting of empirical simulation studies: Introducing the STRESS guidelines. Journal of Simulation, 2018, 13(1), 55–67.\nFortunato, L. and Galassi, M.,The case for free and open source software in research and scholarship. Philosophical Transactions of the Royal Society A, 2021. 379(2197).\nMonks, T., et al., The simulation reproducibility crisis. Can reporting guidelines help? 2017 (Operational Research Society): p. 211-218.\nHudson, R., Should We Strive to Make Science Bias-Free? A Philosophical Assessment of the Reproducibility Crisis. Journal for General Philosophy of Science, 2021. 52(3): p. 389-405.\nBaker, M., 1,500 scientists lift the lid on reproducibility. Nature, 2016. 533(7604): p. 452-454.\nCalnan, M., et al., Understanding and tackling the reproducibility crisis – Why we need to study scientists’ trust in data. Pharmacological Research, 2024. 199: p. 107043."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Check out my latest blog posts",
    "section": "",
    "text": "Reproducibility matters\n\n\nMaking the case for computationally reproducible research\n\n\n\nInformation\n\n\n\n\n\n\n\n\n\nMar 7, 2025\n\n\nTom Slater\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey, I’m Tom! Thanks for checking out my blog!\nI’ll be covering all things public health and simulation on this platform, so keep an eye out for some exciting posts. In the meantime, have a read of what is already out!\nJust a bit of background on me! I am a PhD student at University of Exeter, officially settled in the department of Mathematics and Statistics. My work looks at a hybrid system dynamics and agent-based approach to address inequalities in physical activity interventions. So although on paper I’m a statistician, I have begun to branch out into the fields of data science, OR, public health and policy analysis too. And I hope to cover all areas (at some point) in this blog.\nIf you’d like to get in tough, feel free to drop me an email at tbs204@exeter.ac.uk or reach out on LinkedIn.\nHope you enjoy the blog!\nTom"
  }
]