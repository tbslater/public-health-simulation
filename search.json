[
  {
    "objectID": "posts/computational-reproducibility/computational-reproducibility-blog-post.html",
    "href": "posts/computational-reproducibility/computational-reproducibility-blog-post.html",
    "title": "Reproducibility matters",
    "section": "",
    "text": "Reproducibility is the gold standard of research. I would argue it is not only important, but essential for scientific work and it is something that can certainly be achievable. Keep reading to find out what reproducibility is, why it is crucial and how you can make your own computational work more reproducible.\nTime to read: 4 minutes\n\n\n\nhttps://unsplash.com/photos/a-computer-screen-with-a-bunch-of-code-on-it-ieic5Tq8YMk (C.Ried, 2018)\n\n\nWHAT IS COMPUTATIONAL REPRODUCIBILITY?\nResearch is said to be computationally reproducible if its results can be obtained from the data, methods and code outlined by the author(s) [1].\nIn other words, if another researcher - separate from the original team - can reach the same outputs using the exact same approach, then full reproducibility has been achieved. If only some results can be obtained, we may refer to this as partial reproducibility.\nWHY SHOULD WE BE BOTHERED ABOUT REPRODUCIBILITY?\nZiemann et al. rightly stress the negative implications of work that cannot be reproduced [2] (I highly recommend reading their paper). At the very least, irreproducible research can stunt development. It is much harder to share work with minimal or even no attempt to outline the methods and code. Learning from this becomes impossible. More dangerously, irreproducible work cannot be verified and so its credibility remains doubtful. Without trust in one’s work, increased resistance between research and policymakers is to be expected, and even if the work is accepted, how do we know acting on the conclusions is safe, if it has not been verified? Needless to say, reproducible research is vital.\nHOW CAN COMPUTATIONAL REPRODUCIBILITY BE ASSESSED?\nReproducibility can be assessed by working through a paper’s methods and running any associated code to obtain the key results [1]. If consistent, though not necessarily identical, outputs are achieved, then the research can be deemed reproducible. This should be done within a reasonable time-frame, at the discretion of the individual researcher.\nREPRODUCIBILITY VS REPLICABILITY\nReplicability is often mentioned in conjunction with reproducibility, but the two should not be confused. Replicable work refers to the achievement of consistent results across several studies, possibly using different methods and data [1]. On the contrary, reproducibility is strictly concerned with use of the same methods. Both are certainly important for the advancement of science, but attaining replicable work is somewhat out of a researcher’s control. Reproducible work should, however, always be expected if best practices have been followed.\nTIPS TO IMPROVE COMPUTATIONAL REPRODUCIBILITY\nBeing accountable to even simple rules can drastically improve computational reproducibility. Researchers should [3] (if possible):\n\nShare code and data: code and data should be made publicly available, follow coding guidelines and ought to run without much modification required.\nUse random seeds: for stochastic models, seeds should be used to ensure randomness is consistent.\nSpecify package versions: packages change over time and can render code to become outdated (and thus not reproducible).\nFully detail methods: the computational model should be explained in its entirety.\nProvide an ORCID iD: authors with ORCID iD can be more easily identified and contacted [4].\n\nThis list is certainly not exhaustive. More rigorous reproducibility frameworks exist for specific models and I encourage you to look at any relevant to you. For simulation models check out https://doi.org/10.1080/17477778.2018.1442155 [5].\nFREE AND OPEN SOURCE SOFTWARE\nOpting for free and open source software (FOSS) is essential where reproducibility is concerned. Under FOSS, users are able to share and modify code freely [6]. Naturally, this leads to more collaborative research and crucially allows others to develop and verify computational results. Researchers should therefore aim to use FOSS where possible.\nARE WE IN A STATE OF CRISIS?\nWith a vast existence of irreproducible work, science finds itself deep into a ‘reproducibility crisis’ [7,8], or so people say. But is it really that serious? It certainly was nearly a decade ago: in 2016, Nature found that over 70% of reproduction attempts were failed by scientists [9], highlighting just how bad the problem was. Nine years on and the issue is still very relevant [10], though the extent to which is unclear. Nonetheless, action is still required now and so if it helps to highlight the severity of irreproducible research, then I will gladly agree that we are in a crisis.\nFINAL REMARKS\nI hope this post has convinced you that reproducible work does indeed matter. The reproducibility crisis can be tackled, but it starts with individual accountability. So start now!\nREFERENCES\n\nUnderstanding Reproducibility and Replicability. National Academies of Sciences, Engineering, and Medicine. 2019. Reproducibility and Replicability in Science.\nZiemann, M., Poulain, P., and Bora, A., The five pillars of computational reproducibility: bioinformatics and beyond. Brief Bioinform, 2023. 24(6).\nSandve, G.K., et al., Ten simple rules for reproducible computational research. PLoS Comput Biol, 2013. 9(10): p. e100328\nORCID iDs at Springer Nature. [Accessed: 6 March 2025]; Available from: https://www.springernature.com/gp/researchers/orcid.\nMonks, T., et al. Strengthening the reporting of empirical simulation studies: Introducing the STRESS guidelines. Journal of Simulation, 2018, 13(1), 55–67.\nFortunato, L. and Galassi, M.,The case for free and open source software in research and scholarship. Philosophical Transactions of the Royal Society A, 2021. 379(2197).\nMonks, T., et al., The simulation reproducibility crisis. Can reporting guidelines help? 2017 (Operational Research Society): p. 211-218.\nHudson, R., Should We Strive to Make Science Bias-Free? A Philosophical Assessment of the Reproducibility Crisis. Journal for General Philosophy of Science, 2021. 52(3): p. 389-405.\nBaker, M., 1,500 scientists lift the lid on reproducibility. Nature, 2016. 533(7604): p. 452-454.\nCalnan, M., et al., Understanding and tackling the reproducibility crisis – Why we need to study scientists’ trust in data. Pharmacological Research, 2024. 199: p. 107043."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey, I’m Tom! Thanks for checking out my blog!\nI’ll be covering all things public health and simulation on this platform, so keep an eye out for some exciting posts. In the meantime, have a read of what is already out!\nJust a bit of background on me! I am a PhD student at University of Exeter, officially settled in the department of Mathematics and Statistics. My work looks at a hybrid system dynamics and agent-based approach to address inequalities in physical activity interventions. So although on paper I’m a statistician, I have begun to branch out into the fields of data science, OR, public health and policy analysis too. And I hope to cover all areas (at some point) in this blog.\nIf you’d like to get in tough, feel free to drop me an email at tbs204@exeter.ac.uk or reach out on LinkedIn.\nHope you enjoy the blog!\nTom"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Check out my latest blog posts",
    "section": "",
    "text": "Reproducibility Analysis: Huang et al. (2019)\n\n\nA reproducibility analysis of discrete-event simulation in healthcare\n\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nMar 13, 2025\n\n\nThomas Slater\n\n\n\n\n\n\n\n\n\n\n\n\nReproducibility matters\n\n\nMaking the case for computationally reproducible research\n\n\n\nInformation\n\n\n\n\n\n\n\n\n\nMar 7, 2025\n\n\nTom Slater\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/reproducing-huang-et-al/huang-blog-post.html",
    "href": "posts/reproducing-huang-et-al/huang-blog-post.html",
    "title": "Reproducibility Analysis: Huang et al. (2019)",
    "section": "",
    "text": "This article documents a reproducibility analysis of a 2019 paper by Huang et al. in health simulation [1]. I will make extensive reference to the paper throughout the report and so to give full recognition to the authors, the reference is provided below.\nThe original model code can be found on GitHub [2] and is freely available under the GPL-3.0 licence."
  },
  {
    "objectID": "posts/reproducing-huang-et-al/huang-blog-post.html#introduction",
    "href": "posts/reproducing-huang-et-al/huang-blog-post.html#introduction",
    "title": "Reproducibility Analysis: Huang et al. (2019)",
    "section": "INTRODUCTION",
    "text": "INTRODUCTION\nReproducible research is critical. Publishing work that can be understood, replicated and developed by others is key to accelerating growth in any field. Yet, research as a whole is not reproducible [3], perhaps alarmingly so, and in the interest of this article, modelling and simulation is certainly no exception [4,5].\nReproducibility is even more vital when modelling in a healthcare setting, given the potential ethical implications of the work. When taking the Hippocratic Oath, doctors promise to share their knowledge to advance healthcare, but there is no such mandate for health data scientists. Nonetheless, health data scientists ought to hold an individual responsibility to facilitate the sharing of their work. This can and will only ever be achieved if reproducibility best practices are adhered to.\nIn an attempt to encourage reproducible research in health simulation, I am documenting my findings from a reproducibility analysis on a discrete-event simulation paper. I will introduce the model and discuss my attempts to reproduce the key results later in the article."
  },
  {
    "objectID": "posts/reproducing-huang-et-al/huang-blog-post.html#discrete-event-simulation",
    "href": "posts/reproducing-huang-et-al/huang-blog-post.html#discrete-event-simulation",
    "title": "Reproducibility Analysis: Huang et al. (2019)",
    "section": "DISCRETE-EVENT SIMULATION",
    "text": "DISCRETE-EVENT SIMULATION\nBefore proceeding, I will briefly mention what discrete-event simulation is (henceforth referred to as DES).\nDES is a type of simulation model, typically used in operational research. It models arrivals, exits and internal movements within a system as a network of entities, resources and queues [6]. Individual entities may enter the system, leave the system, occupy a resource or queue for a resource at any given time.\nA fundamental property of DES is that changes to a system can only occur when events are triggered [7]. Thus, time progression is directly dictated by discrete events (hence, discrete-event simulation). Several discrete events may occur at once and so a key area of interest is the order in which DES deals with these [6].\nDES is frequently applied to investigate patient flows within a range of healthcare settings. The paper of focus, ‘Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation’, is one such example."
  },
  {
    "objectID": "posts/reproducing-huang-et-al/huang-blog-post.html#the-model",
    "href": "posts/reproducing-huang-et-al/huang-blog-post.html#the-model",
    "title": "Reproducibility Analysis: Huang et al. (2019)",
    "section": "THE MODEL",
    "text": "THE MODEL\nIn the paper, DES is used to model endovascular clot retrieval (ECR) treatment allocation for acute ischaemic stroke (AIS) patients. AIS patients are in direct competition for resources with elective non-stroke interventional neuroradiology (elective INR), emergency interventional radiology (emergency IR) and elective interventional radiology (elective IR) patients. The system diagram below shows the possible patient pathways, including arrival, queuing, resource occupancy and system exits. For more detailed model information, please refer to the original paper.\n\n\n\nFigure 1 taken from Huang et al., 2019 [1]\n\n\nHuang et al. were interested in how patient waiting times and resource usage varied under different scenarios:\n\nBaseline: standard procedure under specified parameters.\nExclusive-use: elective IR patients can only use the IR angiography machine.\nDouble angio INRs: an extra INR angiography machine in place of an IR one.\n\nThe impact of adding an extra one and two hours to daytime shifts was also trialled under each of these scenarios.\nThe key results of the paper are figure 2-5, as well as a supplementary figure submitted alongside the article. The success of my reproduction attempts will be directly assessed against these five plots."
  },
  {
    "objectID": "posts/reproducing-huang-et-al/huang-blog-post.html#code",
    "href": "posts/reproducing-huang-et-al/huang-blog-post.html#code",
    "title": "Reproducibility Analysis: Huang et al. (2019)",
    "section": "CODE",
    "text": "CODE\nThe R package simmer is used for simulation. The package is designed for DES model implementation and documentation can be found at https://cran.r-project.org/web/packages/simmer/index.html [8].\nIn order for a reproducibility analysis to be performed in the first place, open code must be provided and this has been done by the authors via GitHub (found at https://github.com/shiweih/desECR [2]). Huang et al. ought to be commended for doing this, as this in itself drastically increases the reproducibility of their work. However, this is not necessarily enough; code that is not readable or fails to run is not of much use. Fortunately, I did not find this to be the case. Though additional comments may have been useful, reasonable knowledge of simmer and DES theory is sufficient for comprehension. The function used to run the simulation worked fine and was easily adaptable. A minor criticism is that, to me at least, the purpose of the plotting function was unclear, as none of the plots were relevant to those in the paper. So my own code has removed this and created plotting functions from scratch."
  },
  {
    "objectID": "posts/reproducing-huang-et-al/huang-blog-post.html#results",
    "href": "posts/reproducing-huang-et-al/huang-blog-post.html#results",
    "title": "Reproducibility Analysis: Huang et al. (2019)",
    "section": "RESULTS",
    "text": "RESULTS\nIn line with the original study, I repeated simulations for each scenario thirty times. A reproducibility seed of 11032025 was used, as a nod to the date simulations were run. The amount of time spent on figure reproductions was not pre-specified. I simply kept on going until I had reached a point that reflected a ‘best’ attempt. Whilst it may have been better practice to give myself a timed cut-off, or at least to document some timings, this analysis served as an introduction to DES for me and so lines between learning general DES coding and specific model comprehension and modification often became blurred. Experienced researchers ought to be more rigorous with time spent.\nFIGURE 2\n\n\n\nFigure 2 taken from Huang et al., 2019 [1]\n\n\nMy attempt to recreate figure 2 from the paper was for the most part unsuccessful. The closest plot I produced is not a match to the original, despite some similarities. The density shapes are inconsistent in most cases and specific values are out.\n\n\n\nReproduction of figure 2 from Huang et al., 2019 [1]\n\n\nFIGURE 3\n\n\n\nFigure 3 taken from Huang et al., 2019 [1]\n\n\nReproduction of figure 3 was not achieved. Similar to figure 2, the densities are not consistent with the original plot. On top of this, the results I obtained for the baseline and double angio INR scenarios suggest a slight reduction in the waiting times when additional hours are added to the shift. This is a direct contradiction to a conclusion from the paper: only the exclusive-use scenario was found to see a significant drop in the wait times with one and two hours of extra work.\n\n\n\nReproduction of figure 3 from Huang et al., 2019 [1]\n\n\nFIGURE 4\n\n\n\nFigure 4 taken from Huang et al., 2019 [1]\n\n\nI was close to being able to reproduce figure 4. My resulting plot has a similar shape to the original figure and the general conclusions associated with it can be reached. However, the exact mean values for disability-free life are not quite consistent with those Huang et al. obtained. It is difficult to claim true reproducibility with this being the case.\n\n\n\nReproduction of figure 4 from Huang et al., 2019 [1]\n\n\nFIGURE 5\n\n\n\nFigure 5 taken from Huang et al., 2019 [1]\n\n\nAgain, reproducing figure 5 was very nearly achieved, but not quite. I managed to obtain similar angio INR utilisation percentages when compared to the original. The value for the two angio INR scenario was in fact an exact match, but the other two were slightly out.\n\n\n\nReproduction of figure 5 from Huang et al., 2019 [1]\n\n\nSUPPLEMENTARY FIGURE\n\n\n\nThe supplementary figure associated with Huang et al., 2019 [1]\n\n\nThe methods required to reproduce the supplementary figure were largely similar to those used for figures 2 and 3 and so unsurprisingly reproduction was unsuccessful. I did manage to get somewhat consistent patterns to the paper - i.e. double and triple the number of patients follow a similar trend to the baseline number of patients - but the densities were not a match.\n\n\n\nReproduction of the supplementary figure from Huang et al., 2019 [1]"
  },
  {
    "objectID": "posts/reproducing-huang-et-al/huang-blog-post.html#challenges",
    "href": "posts/reproducing-huang-et-al/huang-blog-post.html#challenges",
    "title": "Reproducibility Analysis: Huang et al. (2019)",
    "section": "CHALLENGES",
    "text": "CHALLENGES\nSEED FOR REPRODUCIBILITY\nThe model included elements of stochasticity, yet these were not controlled for through the use of a random seed. Without a reproducible seed, the output obtained by Huang et al. and my own output were subject to randomness and so results may not always align. It was unclear whether discrepancies in the two sets of results were due to randomness or if it was something else.\nWAITING TIME PROBABILITIES\nProducing figures 2 and 3, as well as the supplementary figure, required the standardisation of mean waiting times. An explanation of how this was carried out was not included in the paper, requiring some of my own guess work to perform this transformation. In all likelihood, the method I used was not the same as the one in the paper, but remains my closest attempt.\nThe approach I used can be summarised as follows:\n\nAssign all non-zero wait times into a set number of bins.\nReassign all values as the midpoint of the bin they are collected in.\nDivide the frequency of values in each bin by the overall number of patients who did not wait at all, so that a ratio is obtained.\nTransform the ratios using the sigmoid function: \\(f(x) = \\frac{1}{1+e^{-r}}\\), where \\(r\\) is the ratio.\n\nMEAN DISABILITY-FREE LIFE DAYS ADDED\nIt was clear from the paper that the output of figure 4 was just a function of the waiting times for ECR patients. What I was unsure about was which resources the waiting time applied to. Initially, I took the average of all resources used by an ECR patient, but this looked very wrong. I eventually settled on waiting times for just the angio INR resource."
  },
  {
    "objectID": "posts/reproducing-huang-et-al/huang-blog-post.html#final-comments",
    "href": "posts/reproducing-huang-et-al/huang-blog-post.html#final-comments",
    "title": "Reproducibility Analysis: Huang et al. (2019)",
    "section": "FINAL COMMENTS",
    "text": "FINAL COMMENTS\nIt should be said that the intention of this article is not to criticise the work of Huang et al. It is merely meant to advocate for the need for reproducibility and to document my attempts to reproduce this particular piece of work. Huang et al. should be commended for their strong research: they have successfully constructed a fairly complex DES model and have achieved some substantial results. Nonetheless, it is my conclusion that their work is not able to be reproduced. Importantly, this is my conclusion. The results obtained are exclusively my attempts and it may be that others can in fact reproduce the figures successfully, but to the best of my ability I believe it cannot be done. So, despite a thorough piece of research, the paper is not easily verified. I would, however, argue that Huang et al. have provided us with something that can be manipulated and developed by others. Their code was very readable and ran well, it was just the specific plots that proved problematic.\nAdvocating for computationally reproducibility, I ought to make sufficient attempts to ensure the results in this article are themselves reproducible. In light of this, I have upload my code to GitHub https://github.com/tbslater/huang-reproducibility-analysis.git."
  },
  {
    "objectID": "posts/reproducing-huang-et-al/huang-blog-post.html#references",
    "href": "posts/reproducing-huang-et-al/huang-blog-post.html#references",
    "title": "Reproducibility Analysis: Huang et al. (2019)",
    "section": "REFERENCES",
    "text": "REFERENCES\n\nHuang, S., et al. Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation. Front Neurol, 2019. 10: p. 653.\nHuang, S., et al. desECR. 2019 [Accessed: 13/03/2025]; Available from: https://github.com/shiweih/desECR.git.\nHudson, R. Should We Strive to Make Science Bias-Free? A Philosophical Assessment of the Reproducibility Crisis. Journal for General Philosophy of Science, 2021. 52(3): p. 389-405.\nMonks, T., et al. The simulation reproducibility crisis. Can reporting guidelines help? 2017. Operational Research Society.\nTaylor, S.J.E., et al. CRISIS, WHAT CRISIS – DOES REPRODUCIBILITY IN MODELING & SIMULATION REALLY MATTER? in 2018 Winter Simulation Conference (WSC). 2018.\nSchriber, T.J. and D.T. Brunner. Inside discrete-event simulation software: How it works and why it matters. in 2007 Winter Simulation Conference. 2007.\nZhang, P., Chapter 19 - Industrial control system simulation routines, in Advanced Industrial Control Technology, P. Zhang, Editor. 2010, William Andrew Publishing: Oxford. p. 781-810.\nUcar, I., B. Smeets, and A. Azcorra, simmer: Discrete-Event Simulation for R. Journal of Statistical Software, 2019. 90(2): p. 1-30."
  }
]